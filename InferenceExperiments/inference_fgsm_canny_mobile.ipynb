{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import albumentations as A\n",
    "import time\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.nn import functional as F\n",
    "from torch import topk\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x177a65690>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelReducer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super(ChannelReducer, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_canny_edge_images(input_data_dir, output_dir_name):\n",
    "    output_path = os.path.join(\"../PerturbedData/fgsm\",output_dir_name)\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path)\n",
    "    os.mkdir(output_path)\n",
    "    print(f\"Created directory: {output_path}\")\n",
    "    \n",
    "    # for each file in the input_data_dir, create the canny edge map of the input\n",
    "    files = os.listdir(input_data_dir)\n",
    "    for file in files:\n",
    "        image = cv2.imread(os.path.join(input_data_dir, file))\n",
    "        grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "        blurred = cv2.GaussianBlur(grayscale, (5,5), 0) # apply Gaussian blur\n",
    "        canny = cv2.Canny(blurred, 30, 150)\n",
    "        cv2.imwrite(os.path.join(output_path, file), canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../PerturbedData/fgsm/fgsm_canny\n"
     ]
    }
   ],
   "source": [
    "# Trying out create_canny_edge_images\n",
    "input_data_dir = \"../PerturbedData/fgsm/fgsm\"\n",
    "output_dir_name = \"fgsm_canny\"\n",
    "create_canny_edge_images(input_data_dir, output_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb image dim: (54, 53, 3)\n",
      "canny image dim: (54, 53, 1)\n",
      "concatenated image dim: (54, 53, 4)\n",
      "concatenated image tensor dim: torch.Size([1, 4, 54, 53])\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"../PerturbedData/fgsm\"\n",
    "image_1 = \"00000.png\"\n",
    "rgb_image =  cv2.imread(os.path.join(dir_name,\"fgsm\" ,image_1))\n",
    "canny_image =  cv2.imread(os.path.join(dir_name,\"fgsm_canny\" ,image_1), cv2.IMREAD_GRAYSCALE)\n",
    "canny_image = canny_image.reshape((canny_image.shape[0], canny_image.shape[1], -1))\n",
    "concatenated_image = cv2.merge([rgb_image, canny_image])\n",
    "print(f\"rgb image dim: {rgb_image.shape}\")\n",
    "print(f\"canny image dim: {canny_image.shape}\")\n",
    "print(f\"concatenated image dim: {concatenated_image.shape}\")\n",
    "\n",
    "concatenated_image_tensor = torch.from_numpy(concatenated_image.transpose((2, 0, 1))).unsqueeze(0).float()\n",
    "print(f\"concatenated image tensor dim: {concatenated_image_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_tensor dim: torch.Size([1, 3, 54, 53])\n",
      "out_image size is: (54, 53, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reducer = ChannelReducer(in_channels=4, out_channels=3, kernel_size=3)\n",
    "\n",
    "output_tensor = reducer(concatenated_image_tensor)\n",
    "print(f\"output_tensor dim: {output_tensor.shape}\")\n",
    "\n",
    "\n",
    "output_array = output_tensor.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "# Convert the NumPy array to a cv2 image\n",
    "output_image = np.transpose(output_array, (1, 2, 0))\n",
    "output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(f\"out_image size is: {output_image.shape}\")\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')  # Optional: Turn off axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(pretrained=True, fine_tune=True, num_classes=10):\n",
    "    model = models.mobilenet_v3_large(pretrained=pretrained)\n",
    "    if fine_tune:\n",
    "        print('[INFO]: Fine-tuning all layers...')\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = True\n",
    "    elif not fine_tune:\n",
    "        print('[INFO]: Freezing hidden layers...')\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = False\n",
    "            \n",
    "    model.classifier[3] = nn.Linear(in_features=1280, out_features=num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(image_type):\n",
    "    print(\"Capturing images of type:\", image_type)\n",
    "    if image_type == \"original\":\n",
    "        pass\n",
    "    elif image_type == \"gaussian_blur\":\n",
    "        pass\n",
    "    elif image_type == \"pgd_attack\":\n",
    "        pass\n",
    "    elif image_type == \"fgsm_attack\":\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "    sign_names_df = pd.read_csv('../PerturbedData/signnames.csv')\n",
    "    class_names = sign_names_df.SignName.tolist()\n",
    "    \n",
    "    # DataFrame for ground truth.\n",
    "    gt_df = pd.read_csv('../PerturbedData/GT-final_test.csv', delimiter=';')\n",
    "    gt_df = gt_df.set_index('Filename', drop=True)\n",
    "    \n",
    "    \n",
    "    model = build_model(pretrained=False,fine_tune=False, num_classes=43).to(device)\n",
    "    model = model.eval()\n",
    "    model.load_state_dict(torch.load('../outputs/MobileNetV3_model.pth', map_location=device)['model_state_dict'])\n",
    "    \n",
    "    # https://github.com/zhoubolei/CAM/blob/master/pytorch_CAM.py\n",
    "    def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "        # Generate the class activation maps upsample to 256x256.\n",
    "        size_upsample = (256, 256)\n",
    "        bz, nc, h, w = feature_conv.shape\n",
    "        output_cam = []\n",
    "        for idx in class_idx:\n",
    "            cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "            cam = cam.reshape(h, w)\n",
    "            cam = cam - np.min(cam)\n",
    "            cam_img = cam / np.max(cam)\n",
    "            cam_img = np.uint8(255 * cam_img)\n",
    "            output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "        return output_cam\n",
    "\n",
    "    def apply_color_map(CAMs, width, height, orig_image):\n",
    "        for i, cam in enumerate(CAMs):\n",
    "            heatmap = cv2.applyColorMap(cv2.resize(cam,(width, height)), cv2.COLORMAP_JET)\n",
    "            result = heatmap * 0.5 + orig_image * 0.5\n",
    "            result = cv2.resize(result, (224, 224))\n",
    "        return result\n",
    "\n",
    "    def visualize_and_save_map(result, orig_image, gt_idx=None, class_idx=None, save_name=None):\n",
    "        # Put class label text on the result.\n",
    "        if class_idx is not None:\n",
    "            cv2.putText(result, f\"Pred: {str(class_names[int(class_idx)])}\", (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        if gt_idx is not None:\n",
    "            cv2.putText(result, f\"GT: {str(class_names[int(gt_idx)])}\", (5, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2,cv2.LINE_AA)\n",
    "        orig_image = cv2.resize(orig_image, (224, 224))\n",
    "        img_concat = cv2.hconcat([np.array(result, dtype=np.uint8), np.array(orig_image, dtype=np.uint8)])\n",
    "        cv2.imshow('Result', img_concat)\n",
    "        cv2.waitKey(1)\n",
    "        if save_name is not None:\n",
    "            if image_type == \"original\":\n",
    "                cv2.imwrite(f\"outputs/test_results/original_imgs/CAM_{save_name}.jpg\", img_concat)\n",
    "            elif image_type == \"gaussian_blur\":\n",
    "                cv2.imwrite(f\"outputs/test_results/gaussian_blur/CAM_{save_name}.jpg\", img_concat)\n",
    "            elif image_type == \"pgd_attack\":\n",
    "                cv2.imwrite(f\"outputs/test_results/pgd_attack/CAM_{save_name}.jpg\", img_concat)\n",
    "            elif image_type == \"fgsm_attack\":\n",
    "                cv2.imwrite(f\"outputs/test_results/fgsm_attack/CAM_{save_name}.jpg\", img_concat)\n",
    "            else:\n",
    "                cv2.imwrite(f\"outputs/test_results/original_subset/CAM_{save_name}.jpg\", img_concat)\n",
    "\n",
    "    # Hook the feature extractor.\n",
    "    # https://github.com/zhoubolei/CAM/blob/master/pytorch_CAM.py\n",
    "    features_blobs = []\n",
    "    def hook_feature(module, input, output):\n",
    "        features_blobs.append(output.data.cpu().numpy())\n",
    "        model._modules.get('features').register_forward_hook(hook_feature)\n",
    "        # Get the softmax weight.\n",
    "        params = list(model.parameters())\n",
    "        weight_softmax = np.squeeze(params[-4].data.cpu().numpy())\n",
    "\n",
    "        # Define the transforms, resize => tensor => normalize.\n",
    "        transform = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),])\n",
    "        \n",
    "        counter = 0\n",
    "        # Run for all the test images.\n",
    "        if image_type == \"original\":\n",
    "            all_images = glob.glob('../input/GTSRB_Final_Test_Images/GTSRB/Final_Test/Images/*.ppm')\n",
    "        elif image_type == \"gaussian_blur\":\n",
    "            all_images = glob.glob('../input/GTSRB_Final_Test_Images/GTSRB/Final_Test/gaussian_blur/*.ppm')\n",
    "        elif image_type == \"pgd_attack\":\n",
    "            all_images = glob.glob('../input/GTSRB_Final_Test_Images/GTSRB/Final_Test/pgd_attack/*.ppm')\n",
    "        elif image_type == \"fgsm_attack\":\n",
    "            all_images = glob.glob('../input/GTSRB_Final_Test_Images/GTSRB/Final_Test/fgsm_attack/*.ppm')\n",
    "        else:\n",
    "            all_images = glob.glob('../input/GTSRB_Final_Test_Images/GTSRB/Final_Test/original_subset/*.ppm')\n",
    "        \n",
    "        correct_count = 0\n",
    "        frame_count = 0 # To count total frames.\n",
    "        total_fps = 0 # To get the final frames per second. \n",
    "        for i, image_path in enumerate(all_images):\n",
    "        # Read the image.\n",
    "            image = cv2.imread(image_path)\n",
    "            orig_image = image.copy()\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width, _ = orig_image.shape\n",
    "            # Apply the image transforms.\n",
    "            image_tensor = transform(image=image)['image']\n",
    "            # Add batch dimension.\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            #TODO: create canny edge image and concatenate on\n",
    "            \n",
    "            # Forward pass through model.\n",
    "            start_time = time.time()\n",
    "            outputs = model(image_tensor.to(device))\n",
    "            end_time = time.time()\n",
    "            # Get the softmax probabilities.\n",
    "            probs = F.softmax(outputs).data.squeeze()\n",
    "            # Get the class indices of top k probabilities.\n",
    "            class_idx = topk(probs, 1)[1].int()\n",
    "            # Get the ground truth.\n",
    "            image_name = image_path.split(os.path.sep)[-1]\n",
    "            gt_idx = gt_df.loc[image_name].ClassId\n",
    "            # Check whether correct prediction or not.\n",
    "            if gt_idx == class_idx:\n",
    "                correct_count += 1\n",
    "            # Generate class activation mapping for the top1 prediction.\n",
    "            CAMs = returnCAM(features_blobs[0], weight_softmax, class_idx)\n",
    "            # File name to save the resulting CAM image with.\n",
    "            save_name = f\"{image_path.split('/')[-1].split('.')[0]}\"\n",
    "            # Show and save the results.\n",
    "            result = apply_color_map(CAMs, width, height, orig_image)\n",
    "            visualize_and_save_map(result, orig_image, gt_idx, class_idx, save_name)\n",
    "            counter += 1\n",
    "            print(f\"Image: {counter}\")\n",
    "            # Get the current fps.\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # Add `fps` to `total_fps`.\n",
    "            total_fps += fps\n",
    "            # Increment frame count.\n",
    "            frame_count += 1\n",
    "\n",
    "        print(f\"{image_type}:Total number of test images: {len(all_images)}\")\n",
    "        print(f\"{image_type}:Total correct predictions: {correct_count}\")\n",
    "        print(f\"{image_type}:Accuracy: {correct_count/len(all_images)*100:.3f}\")\n",
    "\n",
    "        # Close all frames and video windows.\n",
    "        cv2.destroyAllWindows()\n",
    "        # calculate and print the average FPS\n",
    "        avg_fps = total_fps / frame_count\n",
    "        print(f\"Average FPS: {avg_fps:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Specify the type of attack \")\n",
    "    parser.add_argument(\"image_type\", help=\"original, gaussian_blur, pgd_attack, fgsm_attack\")\n",
    "    parser.add_argument(\"dataset_path\", help=\"specify where the data is located\")\n",
    "    args = parser.parse_args()\n",
    "    main(args.image_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
